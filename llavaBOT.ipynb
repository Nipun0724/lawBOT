{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "llX84zK_Nce2",
        "outputId": "e8fdcb43-7bc6-4f3f-de55-fe1076dba900"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 23.2.1 -> 24.0\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "^C\n"
          ]
        }
      ],
      "source": [
        "!pip install -q transformers==4.37.2\n",
        "!pip install bitsandbytes==0.41.3 accelerate==0.25.0\n",
        "!pip install -q git+https://github.com/openai/whisper.git\n",
        "!pip install -q gradio\n",
        "!pip install -q gTTS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4MJ-CRGZQ6bQ"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import BitsAndBytesConfig, pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_8NoAtVVReum"
      },
      "outputs": [],
      "source": [
        "quant_config = BitsAndBytesConfig(\n",
        "    load_in_4bit = True,\n",
        "    bnb_4bit_compute_dtype= torch.float16\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BzdpKMo7SGan"
      },
      "outputs": [],
      "source": [
        "model_id = \"llava-hf/llava-1.5-7b-hf\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "id": "Y89JEvjNSUiX",
        "outputId": "eda62bf9-978d-43f8-b1e7-84a97144d909"
      },
      "outputs": [
        {
          "ename": "RuntimeError",
          "evalue": "No GPU found. A GPU is needed for quantization.",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-c6d9e6dab309>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m pipe = pipeline(\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0;34m\"image-to-text\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel_id\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mmodel_kwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"quantization_config\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mquant_config\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/pipelines/__init__.py\u001b[0m in \u001b[0;36mpipeline\u001b[0;34m(task, model, config, tokenizer, feature_extractor, image_processor, framework, revision, use_fast, token, device, device_map, torch_dtype, trust_remote_code, model_kwargs, pipeline_class, **kwargs)\u001b[0m\n\u001b[1;32m    868\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mframework\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    869\u001b[0m         \u001b[0mmodel_classes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"tf\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtargeted_task\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"tf\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"pt\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtargeted_task\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"pt\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 870\u001b[0;31m         framework, model = infer_framework_load_model(\n\u001b[0m\u001b[1;32m    871\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0mmodel_classes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel_classes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py\u001b[0m in \u001b[0;36minfer_framework_load_model\u001b[0;34m(model, config, model_classes, task, framework, **model_kwargs)\u001b[0m\n\u001b[1;32m    276\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    277\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 278\u001b[0;31m                 \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_class\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    279\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"eval\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    280\u001b[0m                     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/auto/auto_factory.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    564\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_model_mapping\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    565\u001b[0m             \u001b[0mmodel_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_model_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_model_mapping\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 566\u001b[0;31m             return model_class.from_pretrained(\n\u001b[0m\u001b[1;32m    567\u001b[0m                 \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mmodel_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mhub_kwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    568\u001b[0m             )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   3028\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mload_in_8bit\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mload_in_4bit\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3029\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3030\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"No GPU found. A GPU is needed for quantization.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3031\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mis_accelerate_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mis_bitsandbytes_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3032\u001b[0m                 raise ImportError(\n",
            "\u001b[0;31mRuntimeError\u001b[0m: No GPU found. A GPU is needed for quantization."
          ]
        }
      ],
      "source": [
        "pipe = pipeline(\n",
        "    \"image-to-text\",\n",
        "    model=model_id,\n",
        "    model_kwargs={\"quantization_config\": quant_config}\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QckPoYVcUTzb"
      },
      "outputs": [],
      "source": [
        "pipe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rv3gB2a8TfJw"
      },
      "outputs": [],
      "source": [
        "import whisper\n",
        "import gradio as gr\n",
        "import time\n",
        "import warnings\n",
        "import os\n",
        "from gtts import gTTS\n",
        "from PIL import Image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZhnvGz5TUKkZ"
      },
      "outputs": [],
      "source": [
        "image_path = \"../static/skin+problems+2.jpg\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "igXMdRtaVcU3"
      },
      "outputs": [],
      "source": [
        "image = Image.open((image_path))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EwbThVm1Vjay"
      },
      "outputs": [],
      "source": [
        "image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o6ir7N0oVpcM"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk import sent_tokenize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "05jTNeqeV59C"
      },
      "outputs": [],
      "source": [
        "max_new_tokens = 250"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KMP54nwwV_4b"
      },
      "outputs": [],
      "source": [
        "prompt_instructions =\"\"\"\n",
        "Describe the image using as much as detail as possible.\n",
        "You are a helpful AI assistant who is able to answer questions about the image.\n",
        "What is the image all about?\n",
        "Now generate the helpful answer.\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OaV2lJIcWhpI"
      },
      "outputs": [],
      "source": [
        "prompt = \"User: <image>\\n\" + prompt_instructions + \"\\nAssistant:\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mhSVrQD3XTnR"
      },
      "outputs": [],
      "source": [
        "outputs = pipe(image, prompt=prompt, generate_kwargs={\"max_new_tokens\": max_new_tokens})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UySdOLNDYKzz"
      },
      "outputs": [],
      "source": [
        "outputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ufYlWQ75YPhG"
      },
      "outputs": [],
      "source": [
        "for sent in sent_tokenize(outputs[0][\"generated_text\"]):\n",
        "  print(sent)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ylsTL-U5YpaX"
      },
      "outputs": [],
      "source": [
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aShVknlfYx6d"
      },
      "outputs": [],
      "source": [
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oe7vgK7KY1AH"
      },
      "outputs": [],
      "source": [
        "torch.cuda.is_available()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AlOX5hyWY542"
      },
      "outputs": [],
      "source": [
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8JisHZG1ZFDc"
      },
      "outputs": [],
      "source": [
        "print(f\"Using torch {torch.__version__} ({DEVICE})\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SiX-3DgeZSIh"
      },
      "outputs": [],
      "source": [
        "import whisper"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G4n1QeaYZaC4"
      },
      "outputs": [],
      "source": [
        "model = whisper.load_model(\"small\", device=DEVICE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sFYoZ1wQZvXV"
      },
      "outputs": [],
      "source": [
        "print(\n",
        "    f\"Model is {'multilingual' if model.is_multilingual else 'English-only'}\"\n",
        "    f\"and has {sum(np.prod(v.shape) for v in model.parameters()):,} parameters.\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GctMX39hafMQ"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import datetime"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "He-wLvrDZw1F"
      },
      "outputs": [],
      "source": [
        "##Logger file\n",
        "tstamp = datetime.datetime.now()\n",
        "tstamp = str(tstamp).replace(\" \", \"_\")\n",
        "logfile = f\"log_{tstamp}.txt\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D85Sfug0b9uP"
      },
      "outputs": [],
      "source": [
        "def writehistory(text):\n",
        "    with open(logfile, \"a\", encoding='utf-8') as f:\n",
        "      f.write(text)\n",
        "      f.write(\"\\n\")\n",
        "    f.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lk430CYjcTuv"
      },
      "outputs": [],
      "source": [
        "import requests"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nBlUd7LJcbqS"
      },
      "outputs": [],
      "source": [
        "def img2txt(input_text, input_image):\n",
        "\n",
        "    # load the image\n",
        "    image = Image.open(input_image)\n",
        "\n",
        "    writehistory(f\"Input text: {input_text} - Type: {type(input_text)} - Dir: {dir(input_text)}\")\n",
        "    if type(input_text) == tuple:\n",
        "        prompt_instructions = \"\"\"\n",
        "        Describe the image using as much as detail as possible.\n",
        "        You are a helpful AI assistant who is able to answer questions about the image.\n",
        "        What is the image all about?\n",
        "        Now generate the helpful answer.\n",
        "        \"\"\"\n",
        "    else:\n",
        "      prompt_instructions = \"\"\"\n",
        "      Act as an expert in imagery descriptive analysis, using as much detail as possible from the image, respond to the following prompt:\n",
        "      \"\"\"+ input_text\n",
        "\n",
        "    writehistory(f\"prompt_instructions: {prompt_instructions}\")\n",
        "    prompt = \"USER: <image>\\n\" + prompt_instructions + \"\\nASSISTANT:\"\n",
        "\n",
        "    outputs = pipe(image, prompt=prompt, generate_kwargs={\"max_new_tokens\": 200})\n",
        "\n",
        "    #properly extract the response text\n",
        "    if outputs is not None and len(outputs[0][\"generated_text\"]) > 0:\n",
        "        match = re.search(r'ASSISTANT:\\s*(.*)', outputs[0][\"generated_text\"])\n",
        "        if match:\n",
        "            #Extract the text after \"ASSISTANT:\"\n",
        "            reply = match.group(1)\n",
        "        else:\n",
        "            reply = \"No response found.\"\n",
        "    else:\n",
        "      reply = \"No response generated.\"\n",
        "\n",
        "    return reply"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-OceGgwUlgMa"
      },
      "outputs": [],
      "source": [
        "def transcribe(audio):\n",
        "\n",
        "    # Check if the audio input is None or empty\n",
        "    if audio is None or audio == '':\n",
        "        return ('','',None)  # Return empty strings and None audio file\n",
        "\n",
        "    # language = 'en'\n",
        "\n",
        "    audio = whisper.load_audio(audio)\n",
        "    audio = whisper.pad_or_trim(audio)\n",
        "\n",
        "    mel = whisper.log_mel_spectrogram(audio).to(model.device)\n",
        "\n",
        "    _, probs = model.detect_language(mel)\n",
        "\n",
        "    options = whisper.DecodingOptions()\n",
        "    result = whisper.decode(model, mel, options)\n",
        "    result_text = result.text\n",
        "\n",
        "    return result_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5HgQoF_4l5bM"
      },
      "outputs": [],
      "source": [
        "def text_to_speech(text, file_path):\n",
        "    language = 'en'\n",
        "\n",
        "    audioobj = gTTS(text = text,\n",
        "                    lang = language,\n",
        "                    slow = False)\n",
        "\n",
        "    audioobj.save(file_path)\n",
        "\n",
        "    return file_path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cIlOu65_mSu5"
      },
      "outputs": [],
      "source": [
        "import locale"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z5vkHZv4mpBx"
      },
      "outputs": [],
      "source": [
        "print(locale.getlocale())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pgcR5ewWms45"
      },
      "outputs": [],
      "source": [
        "import locale\n",
        "locale.getpreferredencoding = lambda: \"UTF-8\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hwTQQixVm4mh"
      },
      "outputs": [],
      "source": [
        "!ffmpeg -f lavfi -i anullsrc=r=44100:cl=mono -t 10 -q:a 9 -acodec libmp3lame Temp.mp3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MAYwut8pm7KI"
      },
      "outputs": [],
      "source": [
        "import gradio as gr\n",
        "import base64\n",
        "import os\n",
        "\n",
        "# A function to handle audio and image inputs\n",
        "def process_inputs(audio_path, image_path):\n",
        "    # Process the audio file (assuming this is handled by a function called 'transcribe')\n",
        "    speech_to_text_output = transcribe(audio_path)\n",
        "\n",
        "    # Handle the image input\n",
        "    if image_path:\n",
        "        chatgpt_output = img2txt(speech_to_text_output, image_path)\n",
        "    else:\n",
        "        chatgpt_output = \"No image provided.\"\n",
        "\n",
        "    # Assuming 'transcribe' also returns the path to a processed audio file\n",
        "    processed_audio_path = text_to_speech(chatgpt_output, \"Temp3.mp3\")  # Replace with actual path if different\n",
        "\n",
        "    return speech_to_text_output, chatgpt_output, processed_audio_path\n",
        "\n",
        "# Create the interface\n",
        "iface = gr.Interface(\n",
        "    fn=process_inputs,\n",
        "    inputs=[\n",
        "        gr.Audio(sources=[\"microphone\"], type=\"filepath\"),\n",
        "        gr.Image(type=\"filepath\")\n",
        "    ],\n",
        "    outputs=[\n",
        "        gr.Textbox(label=\"Speech to Text\"),\n",
        "        gr.Textbox(label=\"AI Output\"),\n",
        "        gr.Audio(\"Temp.mp3\")\n",
        "    ],\n",
        "    title=\"LLM powered voice assistant\",\n",
        "    description=\"Upload an image and interact via voice input and audio response.\"\n",
        ")\n",
        "\n",
        "# Launch the interface\n",
        "iface.launch(debug=True)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
